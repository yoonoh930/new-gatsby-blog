---
title: "Selection Methods with R"
category: "Wiki"
date: "2020-01-14"
tags:
    - Data Science
    - R
---

## Methods

This wiki article is to demonstrate how to use R for the following statistical methods:

* Best subset selection
* Foward subset selection
* Backward subset selection

with Cross-Validation and K-folds.

## Library and Functions to use

`regsubsets` from `leaps` package can perform all subset methods. For demonstration purpose, dataset `Hitters` from `ISLR` library will be used.

Note that since `regsubsets` does not have `predict()`, one has to define it. 

## Basic Usage



```R
library(leaps)

## Best subset selections
regfit.best <- regsubsets(y ~ x, data, nvmax = maxNumVar) 

## Foward subset selections
regfit.fwd <- regsubsets(y ~ x, data, nvmax = maxNumVar, method = "forward")

## Backward subset selections
regfit.bwd <- regsubsets(y ~ x, data, nvmax = maxNumVar, method = "backward")
```



`nvmax` is the maximum number of variables to explore to. The default value is 8. 

### Summary

Summary of the subset selection model will yield the following, for example:

```R
summary(regfit.best)

Subset selection object
Call: regsubsets.formula(y ~ x, data = data)
num Variables  (and intercept)
           Forced in Forced out
Var1        FALSE         FALSE
Var2        FALSE         FALSE
Var3        FALSE         FALSE

...
..
.

1 subsets of each size up to ...
Selection Algorithm: exhaustive
         Var1 Var2 Var3 
1  ( 1 ) "*"   " "  " "   
2  ( 1 ) " "   "*"  " "   
3  ( 1 ) " "   "*"  "*"  
```



The last part of summary shows that based on the number of variables chosen for each iteration, it shows inclusion of which variable makes the best result. 

You can plot error values as follow:



```R
reg.summary <- summary(regfit.best)
plot(reg.summary$rss, xlab="Numer of Variables", ylab="RSS") # Or $adjr2, $Cp, $bic
```



To see which variables for each iteration visually, you can use `plot(regfit.best, scale = "scale")`. For the scale, there are options of `r2, adjr2, Cp, bic`.



## Cross Validation

As explained before, there is no `predict` function in `regsubsets()` so we have to define one. First, we have to separate training and test



### Training/Test set separation

```R
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(data), rep = TRUE)
test <- (!train)
```



### Function to calculate errors

```R
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[, xvars]%*%coefi
}
```



### Finding optimal coefficients 

```r
var.errors <- rep(NA, numVar)
regfit.best <- regsubsets(y ~ x, data, nvmax = numVar)
for (i in 1:numVar) {
  pred <- predict(regfit.best, newdata, id = i)
  var.errors[i] = mean((testData - pred)^2)
}
minError_index <- which.man(var.errors)
coef(regfit.best, minError_index)
```



## K-folds

This one also uses predict function defined above.



### Preparation

```R
## 10-folds
k <- 10 
set.seed(1)
folds <- sample(1:k, nrow(data), replace = TRUE)
cv.errors <- matrix(NA, k, numVar, dimnames = list(NULL, paste(1:numVar)))
```



### Calculation

```R
for (j in 1:k) {
  best.fit <- regsubsets(y ~ x, data, nvmax = numVar)
  for (i in 1:numVar) {
    pred <- predict(best.fit, newdata, id = i)
    cv.errors[j, i] <- mean((newdata$y[folds == j] - pred)^2)
  }
}
mean.cv.errors <- apply(cv.errors, 2, mean)
```



### Finding optimal coefficients

```R
reg.best <- regsubsets(y ~ x, data, nvmax = numVar)
coef(reg.best, which.min(mean.cv.errors))
```



Source: Introduction to Statistical Learning with Application in R